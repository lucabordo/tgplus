This folder needs to be populated, manually at the moment, with the "movies_metadata.csv" file.
Other intermediate data generated by the project, like model weights, will also be saved under this folder. 

We could have the data from Kaggle auto-populated - download using HTTP and unzip, on first use;
or using the kaggle Python library (https://github.com/Kaggle/kaggle-api) that makes these installs easier. 
Either way there are credentials to pass which break a complete "runs out of the box" ideal.

## Note on data testing

Given that the CSV is not enormous, we could include it in the codebase, but I'm against including data 
inside a Git repo - data access and data versioning are a different thing from source control.

There is sometimes value in including very small test data in the repo, but this also doesn't ensure that any code
actually tied to data-loading specific datasets will work against the read data. I won't do that here. 
Usually I have:
- Some code that's tied to specific data and exposes that data for programmatic use, e.g. "KaggleMovieDataset1"
  this dataloading code is inherently tied to the data, and only at load time of the actual data source 
  can we make assertions regarding the version of the data, and the correctness of data loading.
- Some code that uses the data loaded from various sources. These data can have a common interface;
  any code that uses such data can be tested by using very small and simple mock data that abides the interface.
  The word "interface" here doesn't need to refer to something complicated - just some well-defined type
  or table schema (e.g. "A datatable that has string columns for 'genre' and 'overview'")


